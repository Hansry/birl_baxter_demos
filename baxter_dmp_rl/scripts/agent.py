#!/usr/bin/env python
import math
import random


class Agent(object):
	'''
	this is a base RL Agent class
	'''
	def __init__(self, env):
		self.env = env
		self.state = None
		self.next_waypoint = None
		self.color = None
		self.self.primary_agent = False

	def reset(self, destination = None, testing = False):
		pass

	def update(self):
		pass

	def get_state(self):
		pass

	def get_next_waypoint(self):
		pass

class LearningAgent(Agent):
	'''
	This is a baxter DMP-RL agent that learns to grasp a object 
	'''
	def __init__(self, learning = False, epsilon = 1.0, alpha = 0.50, tolerance = 0.5):
		#super(LearningAgent, self).__init__(env)          # Set the agent in the environment
		#self.planner = RoutePlanner(self.env, self)     # Create a route planner
		self.valid_actions =  [    # The set of valid actions
					"/home/karen/ros/indigo/baxter_ws/src/baxter_dmp/birl_baxter_dmp/dmp/datasets/baxter_joint_output_data0.csv",
	 				"/home/karen/ros/indigo/baxter_ws/src/baxter_dmp/birl_baxter_dmp/dmp/datasets/baxter_joint_output_data1.csv",
	  				"/home/karen/ros/indigo/baxter_ws/src/baxter_dmp/birl_baxter_dmp/dmp/datasets/baxter_joint_output_data2.csv",
	   				"/home/karen/ros/indigo/baxter_ws/src/baxter_dmp/birl_baxter_dmp/dmp/datasets/baxter_joint_output_data3.csv",
	    				"/home/karen/ros/indigo/baxter_ws/src/baxter_dmp/birl_baxter_dmp/dmp/datasets/baxter_joint_output_data4.csv"]    

		# Set parameters of the learning agent
		self.learning = learning                                	# Whether the agent is expecten to learn
		self.Q = dict()                                            	# Create a Q-table which will be a dictionary of tuples 
		self.epsilon = epsilon                                     	# Random exploration factor
		self.alpha = alpha                                          	# Learning factor or learning rate
		self.tolerance = tolerance

	def reset(self, destination = None, testing = False):
		'''
		The reset function is called at the beginning of each trial.
		'testing' is set to True if testing trials are being used
		once training trals have completed
		'''
		#self.planner.route_to(destination)
		# Update epsilon using a decay function of your choice
		# Update additional calss parameters as needed
		# If 'testing' is True, set epsilon and alpha to 0
		self.epsilon = 0.90 * self.epsilon

		return None

	def build_state(self):
		''' The build_state function is called when the agent requests data from the 
		environment. The next waypoint, the intersection inputs, and the deadline
		are all features available to the agent
		'''

		# Collect data about the environment
		#waypoint = self.planner.next_waypoint()         	# The next waypoint
		#inputs = self.env.sense(self)                            	# Visal input - intersection light and traffic 
		#deadline = self.env.get_deadline(self)              # Remaining deadline

		# Set 'state' as a tuple of relevant data for the agent
		#state = (waypoint, tuple(inputs.values()))
		state = ('only_one_state')

		return state

	def get_maxQ(self, state):
		'''
		The get_maxQ function is called when the agent is asked to find the 
		maximum Q-value of all actions based on the 'state' the Baxter DMP is in
		'''

		maxQ = max(self.Q[state].values())

		return maxQ

	def createQ(self, state):
		'''
		The createQ function is called when a state is generated by the agent
		'''

		if self.learning == True:
			if state not in self.Q.keys():
				self.Q[state] = {action_name : 0.0 for action_name in self.valid_actions}
			## Or try this code 
			#for k in self.valid_actions :
			#	q_value_dict = {k : 0.0}
			#self.Q[state] = q_value_dict

		return

	def choose_action(self, state):
		'''The choose_action function is called when the agent is asked to choose
		which action to take, based on the 'state' the Baxter DMP is in.
		'''
		#1. When not learning, choose a random action
		#2. When learning, choose a random action with 'epsilon' probability
		#3. Otherwise, choose an action with the hightest Q-value for the current state

		self.state = state

		action = None
		Qmax_action = list()
		# When not learning, choose a random action
		if self.learning == False:
			action = random.choice(self.valid_actions)

		# When learning, choose an action with 'epsilon' probability
		if self.learning == True:
			if random.random() < self.epsilon:
				action = random.choice(self.valid_actions)
			else:
			# Otherwise, choose an action with the max Q-value and '1-epsilon' probability
				for Q_action, Q_reward in self.Q[state].iteritems():
					if Q_reward == self.get_maxQ(state):
						# We may have the more than 1 max Q-value action 
						Qmax_action.append(Q_action)
				# random choice a max Q-value actions
				action = random.choice(Qmax_action)

		return action

	def learn(self, state, action, reward):
		'''The learn function is called when the agen completes an action and 
		receives an award. This function dose not consider future rewards
		when conducting learning
		'''

		# When learning, implement the value iteration update rule
		# Use only the learning rate 'alpha' (DO NOT use the discount factor 'gamma')

		# This is Q-learning value iteration function
		# Q(s_t, a_t) <- (1 - alpha) * Q(s_t, a_t) + alpha * (r_t+1 + gamma * max_a'{Q(s_t+1, a')})
		# a' are all possiable acations
		if self.learning == True:
			self.Q[state].update({action : (1 - self.alpha) * self.Q[state][action] + self.alpha * reward })

		return

	def update(self, env_reward):
		'''
		The update function is called when a time step is completed in the 
		environment for a given tral. This function will build the agent state,
		choose an action, receive a reward, and learn if enabled.
		'''

		state = self.build_state()                    	# Get current state
		self.createQ(state)                            	# Create 'state' in Q-table
		action = self.choose_action(state)     	# Choose an action
		reward = env_reward     		# Receive a reward
		self.learn(state, action, reward)        	# Q-learn

		return




			





